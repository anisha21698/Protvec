{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishayallapragada/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr 29 17:15:44 2021\n",
    "\n",
    "@author: anishayallapragada\n",
    "\"\"\"\n",
    "\n",
    "from Bio import SeqIO\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "\"\"\"\n",
    " 'AGAMQSASM' => [['AGA', 'MQS', 'ASM'], ['GAM','QSA'], ['AMQ', 'SAS']]\n",
    "\"\"\"\n",
    "def split_ngrams(seq, n):\n",
    "    a, b, c = zip(*[iter(seq)]*n), zip(*[iter(seq[1:])]*n), zip(*[iter(seq[2:])]*n)\n",
    "    str_ngrams = []\n",
    "    for ngrams in [a,b,c]:\n",
    "        x = []\n",
    "        for ngram in ngrams:\n",
    "            x.append(\"\".join(ngram))\n",
    "        str_ngrams.append(x)\n",
    "    return str_ngrams\n",
    "\n",
    "\n",
    "'''\n",
    "Args:\n",
    "    corpus_fname: corpus file name\n",
    "    n: the number of chunks to split. In other words, \"n\" for \"n-gram\"\n",
    "    out: output corpus file path\n",
    "Description:\n",
    "    Protvec uses word2vec inside, and it requires to load corpus file\n",
    "    to generate corpus.\n",
    "'''\n",
    "def generate_corpusfile(corpus_fname, n, out):\n",
    "    f = open(out, \"w\")\n",
    "    with gzip.open(corpus_fname, 'rb') as fasta_file:\n",
    "        for r in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            ngram_patterns = split_ngrams(r.seq, n)\n",
    "            for ngram_pattern in ngram_patterns:\n",
    "                f.write(\" \".join(ngram_pattern) + \"\\n\")\n",
    "                sys.stdout.write(\".\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def load_protvec(model_fname):\n",
    "    return word2vec.Word2Vec.load(model_fname)\n",
    "\n",
    "def normalize(x):\n",
    "    return x / np.sqrt(np.dot(x, x))\n",
    "\n",
    "class ProtVec(word2vec.Word2Vec):\n",
    "\n",
    "    \"\"\"\n",
    "    Either fname or corpus is required.\n",
    "\tcorpus_fname: fasta file for corpus\n",
    "    corpus: corpus object implemented by gensim\n",
    "    n: n of n-gram\n",
    "    out: corpus output file path\n",
    "    min_count: least appearance count in corpus. if the n-gram appear k times which is below min_count, the model does not remember the n-gram\n",
    "    \"\"\"\n",
    "    def _init_(self, corpus_fname=None, corpus=None, n=3, size=100,\n",
    "                   sg=1, window=25, min_count=1, workers=3):\n",
    "        skip_gram = True \n",
    "        \n",
    "        self.n = n\n",
    "        self.size = size\n",
    "        self.corpus_fname = corpus_fname\n",
    "        self.sg = int(skip_gram)\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        out = corpus.txt\n",
    "\n",
    "        directory = out.split('/')[0]\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(\"directory(trained_models) created\\n\")\n",
    "\n",
    "        if corpus is None and corpus_fname is None:\n",
    "            raise Exception(\"Either corpus_fname or corpus is needed!\")\n",
    "\n",
    "        if corpus_fname is not None:\n",
    "            print ('Now we are checking whether corpus file exist')\n",
    "            if not os.path.isfile(out):\n",
    "                print ('INFORM : There is no corpus file. Generate Corpus file from fasta file...')\n",
    "                generate_corpusfile(corpus_fname, n, out)\n",
    "            else:\n",
    "                print( \"INFORM : File's Existence is confirmed\")\n",
    "            self.corpus = word2vec.Text8Corpus(out)\n",
    "            print (\"\\n... OK\\n\")\n",
    "\n",
    "    def word2vec_init_(self, ngram_model_fname):\n",
    "        word2vec.Word2Vec._init_(self, self.corpus, size=self.size, sg=self.sg, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        model = word2vec.Word2Vec([line.rstrip().split() for line in open(self.out)], min_count = 1, size=self.size, sg=self.sg, window=self.window)\n",
    "        model.wv.save_word2vec_format(ngram_model_fname)\n",
    "    \n",
    "    def to_vecs(self, seq, ngram_vectors):\n",
    "        ngrams_seq = split_ngrams(seq, self.n)\n",
    "\n",
    "\n",
    "        protvec = np.zeros(self.size, dtype=np.float32)\n",
    "        for index in xrange(len(seq) + 1 - self.n):\n",
    "            ngram = seq[index:index + self.n]\n",
    "            if ngram in ngram_vectors:\n",
    "                ngram_vector = ngram_vectors[ngram]\n",
    "                protvec += ngram_vector\n",
    "        return normalize(protvec)\n",
    "        \n",
    "    def get_ngram_vectors(self, file_path):\n",
    "        ngram_vectors = {}`\n",
    "        vector_length = None\n",
    "        with open(file_path) as infile:\n",
    "            for line in infile:\n",
    "                line_parts = line.rstrip().split()   \n",
    "                # skip first line with metadata in word2vec text file format\n",
    "                if len(line_parts) > 2:     \n",
    "                    ngram, vector_values = line_parts[0], line_parts[1:]          \n",
    "                    ngram_vectors[ngram] = np.array(map(float, vector_values), dtype=np.float32)\n",
    "        return ngram_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "\n",
    "def make_protein_vector_for_uniprot(fasta_file, protein_vector_fname, ngram_vectors):\n",
    "    with gzip.open(fasta_file, 'rb') as fasta_file:\n",
    "        with open(protein_vector_fname, 'w') as output_file:\n",
    "            for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                protein_name = record.name.split('|')[-1]\n",
    "                protein_vector = pv.to_vecs(record.seq, ngram_vectors)\n",
    "\n",
    "                output_file.write('{}\\t{}\\n'.format(protein_name, ' '.join(map(str, protein_vector))))\n",
    "\n",
    "def make_protein_vector_for_other(fasta_file, protein_vector_fname, ngram_vectors):\n",
    "    with gzip.open(fasta_file, 'rb') as fasta_file:\n",
    "        with open(protein_vector_fname, 'w') as output_file:\n",
    "            for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                protein_name = record.name.split(' ')[-1]\n",
    "                protein_vector = pv.to_vecs(record.seq, ngram_vectors)\n",
    "\n",
    "                output_file.write('{}\\t{}\\n'.format(protein_name, ' '.join(map(str, protein_vector))))\n",
    "\n",
    "def get_uniprot_protein_families(path):\n",
    "    protein_families = {}\n",
    "    protein_family_stat = Counter()\n",
    "    for record in SeqIO.parse(path, \"fasta\"): \n",
    "        family_id = None\n",
    "        for element in record.description.split():\n",
    "            if element.startswith('PFAM'):\n",
    "                family_id = element.split('=', 1)[1]\n",
    "        if family_id:\n",
    "            uniprot_id = record.name.split('|')[-1]\n",
    "            protein_families[uniprot_id] = family_id\n",
    "            protein_family_stat[family_id] += 1\n",
    "\n",
    "    return protein_families, protein_family_stat\n",
    "\n",
    "def make_uniport_with_families(Pfam_file, fasta_file, uniprot_with_families): \n",
    "    protein_families = {}\n",
    "    protein_family_stat = Counter()\n",
    "    with gzip.open(Pfam_file, 'rb') as gzipped_file:\n",
    "        for record in SeqIO.parse(gzipped_file, \"fasta\"):  \n",
    "            family_id = record.description.rsplit(';', 2)[-2]\n",
    "            uniprot_id = record.name.split('/', 1)[0].lstrip('>') \n",
    "            protein_families[uniprot_id] = family_id\n",
    "\n",
    "    with gzip.open(fasta_file, 'rb') as gzipped_file, open(uniprot_with_families, \"w\") as output_fasta:\n",
    "        for record in SeqIO.parse(gzipped_file, \"fasta\"):\n",
    "            uniprot_id = record.name.split('|')[2] \n",
    "            if uniprot_id in protein_families:\n",
    "                family = protein_families[uniprot_id]\n",
    "                record.description += ' PFAM={}'.format(protein_families[uniprot_id])\n",
    "                SeqIO.write(record, output_fasta, \"fasta\")\n",
    "\n",
    "def make_protein_pfam_vector_for_uniprot(protein_pfam_vector_fname, protein_vector_fname, protein_families, protein_family_stat):\n",
    "    #Cut standard\n",
    "    min_proteins_in_family = 100\n",
    "\n",
    "    f = open(protein_pfam_vector_fname, \"w\")\n",
    "    with open(protein_vector_fname) as protein_vector_file:\n",
    "        for line in protein_vector_file:\n",
    "            uniprot_name, vector_string = line.rstrip().split('\\t', 1)\n",
    "            if uniprot_name in protein_families:\n",
    "                family = protein_families[uniprot_name]\n",
    "                if protein_family_stat[family] >= min_proteins_in_family:\n",
    "                    f.write('{},{},{}'.format(uniprot_name, protein_families[uniprot_name], vector_string) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def make_protein_pfam_vector_for_other(protein_pfam_vector_fname, protein_vector_fname, fasta_file):\n",
    "    #Cut standard\n",
    "    min_proteins_in_family = 0\n",
    "\n",
    "    protein_families = {}\n",
    "    f = open(protein_pfam_vector_fname, \"w\")\n",
    "    with open(protein_vector_fname) as protein_vector_file, gzip.open(fasta_file, 'rb') as gzipped_fasta:\n",
    "        for record in SeqIO.parse(gzipped_fasta, \"fasta\"):\n",
    "            gz_protein_name, gz_family = record.description.rstrip().split(' ', 1)\n",
    "            print (gz_protein_name)\n",
    "            print (gz_family)\n",
    "            protein_families[gz_protein_name] = gz_family\n",
    "        \n",
    "        for line in protein_vector_file:\n",
    "            protein_name, vector_string = line.rstrip().split('\\t', 1)\n",
    "            if protein_name in protein_families:\n",
    "                family = protein_families[protein_name]\n",
    "                f.write('{}\\t{}\\t{}'.format(protein_name, protein_families[protein_name], vector_string) + \"\\n\")\n",
    "                \n",
    "    f.close()\n",
    "\n",
    "fasta_file = \"document/uniprot.fasta.gz\"\n",
    "Pfam_file = \"document/Pfam-A.fasta.gz\"\n",
    "ngram_corpus_fname = \"trained_models/ngram_vector.csv\"\n",
    "model_ngram = \"trained_models/ngram_model\"\n",
    "protein_vector_fname = \"trained_models/protein_vector.csv\"\n",
    "uniprot_with_families = \"trained_models/uniprot_with_families.fasta\"\n",
    "protein_pfam_vector_fname = \"trained_models/protein_pfam_vector.csv\"\n",
    "\n",
    "#Make corpus\n",
    "\n",
    "pv = Word2Vec.ProtVec(fasta_file)\n",
    "\n",
    "print (\"Checking the file(trained_models/ngram_vector.csv)\")\n",
    "if not os.path.isfile(ngram_corpus_fname) or not os.path.isfile(protein_vector_fname):\n",
    "    print ('INFORM : There is no vector model file. Generate model files from data file...')\n",
    "    \n",
    "    #Make ngram_vector.txt and word2vec model\n",
    "    pv.word2vec_init_(ngram_corpus_fname)\n",
    "    pv.save(model_ngram) \n",
    "\n",
    "    #Get ngram and vectors\n",
    "    ngram_vectors = pv.get_ngram_vectors(ngram_corpus_fname)\n",
    "    \n",
    "    #Make protein_vector.txt by ngram, vector, uniprot\n",
    "    make_protein_vector_for_uniprot(fasta_file, protein_vector_fname, ngram_vectors)\n",
    "\n",
    "else:\n",
    "    print(\"INFORM : File's Existence is confirmed\\n\")\n",
    "\n",
    "print (\"...OK\\n\")\n",
    "\n",
    "print(\"Checking the file(trained_models/protein_pfam_vector.csv)\")\n",
    "if not os.path.isfile(protein_pfam_vector_fname):\n",
    "    print ('INFORM : There is no pfam_model file. Generate pfam_model files from data file...')\n",
    "    \n",
    "    #Make uniprot_with_family.fasta by uniprot, Pfam\n",
    "    make_uniport_with_families(Pfam_file, fasta_file, uniprot_with_families)\n",
    "\n",
    "    #Get protein_name, family_name, vectors\n",
    "    protein_families, protein_family_stat = get_uniprot_protein_families(uniprot_with_families)\n",
    "\n",
    "    #Make protein_pfam_vector_fname.csv by protein_name, family_name, vectors\n",
    "    make_protein_pfam_vector_for_uniprot(protein_pfam_vector_fname, protein_vector_fname, protein_families, protein_family_stat)\n",
    "\n",
    "print (\"...Uniprot Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
