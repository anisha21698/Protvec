{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishayallapragada/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr 29 17:15:44 2021\n",
    "\n",
    "@author: anishayallapragada\n",
    "\"\"\"\n",
    "\n",
    "from Bio import SeqIO\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "\"\"\"\n",
    " 'AGAMQSASM' => [['AGA', 'MQS', 'ASM'], ['GAM','QSA'], ['AMQ', 'SAS']]\n",
    "\"\"\"\n",
    "def split_ngrams(seq, n):\n",
    "    a, b, c = zip(*[iter(seq)]*n), zip(*[iter(seq[1:])]*n), zip(*[iter(seq[2:])]*n)\n",
    "    str_ngrams = []\n",
    "    for ngrams in [a,b,c]:\n",
    "        x = []\n",
    "        for ngram in ngrams:\n",
    "            x.append(\"\".join(ngram))\n",
    "        str_ngrams.append(x)\n",
    "    return str_ngrams\n",
    "\n",
    "\n",
    "'''\n",
    "Args:\n",
    "    corpus_fname: corpus file name\n",
    "    n: the number of chunks to split. In other words, \"n\" for \"n-gram\"\n",
    "    out: output corpus file path\n",
    "Description:\n",
    "    Protvec uses word2vec inside, and it requires to load corpus file\n",
    "    to generate corpus.\n",
    "'''\n",
    "def generate_corpusfile(corpus_fname, n, out):\n",
    "    f = open(out, \"w\")\n",
    "    with gzip.open(corpus_fname, 'rb') as fasta_file:\n",
    "        for r in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            ngram_patterns = split_ngrams(r.seq, n)\n",
    "            for ngram_pattern in ngram_patterns:\n",
    "                f.write(\" \".join(ngram_pattern) + \"\\n\")\n",
    "                sys.stdout.write(\".\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def load_protvec(model_fname):\n",
    "    return word2vec.Word2Vec.load(model_fname)\n",
    "\n",
    "def normalize(x):\n",
    "    return x / np.sqrt(np.dot(x, x))\n",
    "\n",
    "class ProtVec(word2vec.Word2Vec):\n",
    "\n",
    "    \"\"\"\n",
    "    Either fname or corpus is required.\n",
    "\tcorpus_fname: fasta file for corpus\n",
    "    corpus: corpus object implemented by gensim\n",
    "    n: n of n-gram\n",
    "    out: corpus output file path\n",
    "    min_count: least appearance count in corpus. if the n-gram appear k times which is below min_count, the model does not remember the n-gram\n",
    "    \"\"\"\n",
    "    def _init_(self, corpus_fname=None, corpus=None, n=3, size=100,\n",
    "                   sg=1, window=25, min_count=1, workers=3):\n",
    "        skip_gram = True\n",
    "\n",
    "        self.n = n\n",
    "        self.size = size\n",
    "        self.corpus_fname = corpus_fname\n",
    "        self.sg = int(skip_gram)\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        out = corpus.txt\n",
    "\n",
    "        directory = out.split('/')[0]\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(\"directory(trained_models) created\\n\")\n",
    "\n",
    "        if corpus is None and corpus_fname is None:\n",
    "            raise Exception(\"Either corpus_fname or corpus is needed!\")\n",
    "\n",
    "        if corpus_fname is not None:\n",
    "            print ('Now we are checking whether corpus file exist')\n",
    "            if not os.path.isfile(out):\n",
    "                print ('INFORM : There is no corpus file. Generate Corpus file from fasta file...')\n",
    "                generate_corpusfile(corpus_fname, n, out)\n",
    "            else:\n",
    "                print( \"INFORM : File's Existence is confirmed\")\n",
    "            self.corpus = word2vec.Text8Corpus(out)\n",
    "            print (\"\\n... OK\\n\")\n",
    "\n",
    "    def word2vec_init_(self, ngram_model_fname):\n",
    "        word2vec.Word2Vec._init_(self, self.corpus, size=self.size, sg=self.sg, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        model = word2vec.Word2Vec([line.rstrip().split() for line in open(self.out)], min_count = 1, size=self.size, sg=self.sg, window=self.window)\n",
    "        model.wv.save_word2vec_format(ngram_model_fname)\n",
    "    \n",
    "    def to_vecs(self, seq, ngram_vectors):\n",
    "        ngrams_seq = split_ngrams(seq, self.n)\n",
    "\n",
    "\n",
    "        protvec = np.zeros(self.size, dtype=np.float32)\n",
    "        for index in xrange(len(seq) + 1 - self.n):\n",
    "            ngram = seq[index:index + self.n]\n",
    "            if ngram in ngram_vectors:\n",
    "                ngram_vector = ngram_vectors[ngram]\n",
    "                protvec += ngram_vector\n",
    "        return normalize(protvec)\n",
    "        \n",
    "    def get_ngram_vectors(self, file_path):\n",
    "        ngram_vectors = {}\n",
    "        vector_length = None\n",
    "        with open(file_path) as infile:\n",
    "            for line in infile:\n",
    "                line_parts = line.rstrip().split()   \n",
    "                # skip first line with metadata in word2vec text file format\n",
    "                if len(line_parts) > 2:     \n",
    "                    ngram, vector_values = line_parts[0], line_parts[1:]          \n",
    "                    ngram_vectors[ngram] = np.array(map(float, vector_values), dtype=np.float32)\n",
    "        return ngram_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
